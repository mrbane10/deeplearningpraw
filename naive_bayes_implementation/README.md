# Naive Bayes Spam Email Classifier

A from-scratch implementation of Naive Bayes for email spam detection, demonstrating the critical importance of numerical stability in machine learning.

## ğŸ¯ Key Achievement

**Improved accuracy from 84.82% to 99.21%** by implementing log-space computations to handle numerical underflow.

## ğŸ“Š Results

| Implementation | Accuracy | Precision | Recall | F1-Score |
|---------------|----------|-----------|--------|----------|
| Standard NB   | 84.82%   | 59.57%    | 98.03% | 74.11%   |
| **Log-space NB** | **99.21%** | **98.42%** | 98.03% | **98.22%** |

**Dataset**: 5,728 emails (23.88% spam, 76.12% ham)

## ğŸ” The Problem & Solution

**Problem**: Standard Naive Bayes suffers from numerical underflow when multiplying many small probabilities, causing them to become zero and breaking classification.

**Solution**: Log-space computations convert multiplication to addition, maintaining numerical precision:
- Standard: P(Email|Class) = âˆ P(word|Class) â†’ underflows to 0
- Log-space: log P(Email|Class) = Î£ log P(word|Class) â†’ stays precise

## ğŸ› ï¸ Features

- **From-scratch Naive Bayes** with Laplace smoothing
- **Text preprocessing** (tokenization, stopword removal)
- **Numerical stability** comparison
- **Interactive demo** with 100% accuracy on test cases
- **Comprehensive evaluation** metrics

## ğŸš€ Quick Start

```python
# Install dependencies
pip install numpy pandas nltk

# Run classification
email = "Congratulations! You've won $1000! Click here now!"
result = classify_email(email)  # Returns: "Spam"
